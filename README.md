We progressively build a large language model starting from a simple bigram model.

## Notebooks

- **bigram.ipynb**: Implementation and experiments with a simple bigram language model.
- **MLP.ipynb**: Building and training a multi-layer perceptron for sequence modeling.
- **optimization.ipynb**: Techniques and experiments in optimization for neural networks.
- **backprop_ninja.ipynb**: Some backpropagation and autodiff exercises.
- **WaveNet.ipynb**: Exploring the WaveNet architecture for generative modeling.
