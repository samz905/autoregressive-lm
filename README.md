We progressively build a large language model starting from a simple bigram model.

## Notebooks

- **[bigram.ipynb](bigram.ipynb)**: Implementation of a simple bigram language model.
- **[MLP.ipynb](MLP.ipynb)**: Building and training a multi-layer perceptron for sequence modeling.
- **[optimization.ipynb](optimization.ipynb)**: Techniques in optimization for neural networks such as batch norm.
- **[backprop_ninja.ipynb](backprop_ninja.ipynb)**: Some backpropagation and autodiff exercises.
- **[WaveNet.ipynb](WaveNet.ipynb)**: Exploring the WaveNet architecture for character level modeling.
